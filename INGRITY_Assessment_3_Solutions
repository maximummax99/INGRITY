import os
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.types import DecimalType
from pyspark.sql.window import Window
from pyspark.sql.functions import col, count, avg, min, max, collect_list, concat_ws, lit
import pandas as pd
import pyodbc
from sqlalchemy import create_engine, URL
import streamlit as st


if __name__ == "__main__":
    spark = SparkSession.Builder().appName("Testing").master("local[4]").getOrCreate()


    #1) Read the Input_data CSV file into pyspark dataframe using.
    df = spark.read.csv("C:/Users/Max Sun/Downloads/Input_data.csv", header = True, inferSchema = True , quote = '"')
    df.show(10, False)


    #2) Create a new dataframe "df_out" by adding new column "Web_TREE" as shown in the output_data sheet.
    window_spec = Window.partitionBy("BRAND_ID").orderBy("PARENT_CATEGORY_ID")

    df_out = df.withColumn("Web_TREE", concat_ws("_", collect_list("PARENT_CATEGORY_ID").over(window_spec)))

    df_out = df_out.withColumnRenamed("DVISION_ID", "DIVISION_IDS")
    df_out.show(1000, truncate=False)


    #3) Write the "df_out" dataframe as JSON file in "./out/json" directory.
    df_out.write.json("C:/Users/Max Sun/Downloads/out/json")


    #4) Write the "df_out" dataframe as Parquet file in "./out/parquet" directory.
    df_out.write.parquet("C:/Users/Max Sun/Downloads/out/parquet")


